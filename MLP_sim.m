%% 15 Neurocomputing: MLP w/ multiple hidden layers (Vectorized)
%% Code generated by dkim & jkung

% clc;
% clear all;
% close all;

%% Training data preparation
global BENCHMARK;
%  BENCHMARK = 'MNIST';
% BENCHMARK = 'CIFAR10';
% BENCHMARK = 'LETTER';
% BENCHMARK = 'CNAE-9';
BENCHMARK = 'SPAM';


if strcmp(BENCHMARK, 'MNIST')
    addpath ../RAW_DATA/mnist/
    [ mnistData, mnistLabels ] 	= loadMNIST();
    
    % one-hot encoding
    temp_labels = onehotEncode(mnistLabels);
    
    num_train 	= 50000;
    trainData 	= mnistData(:,1:num_train);
    trainLabels = temp_labels(:,1:num_train);
    
    test_range  = [50001:60000];
    testData    = mnistData(:,test_range);
    testLabels  = temp_labels(:,test_range);
    testLabels_one = mnistLabels(test_range);
elseif strcmp(BENCHMARK, 'CIFAR10')
    addpath ../RAW_DATA/cifar-10/
    [ trainData, trainLabels, testData, testLabels_one ] = loadDataSet();     % 50000 train data
    trainData   = trainData';
    testData    = testData';
    
    trainLabels = onehotEncode(trainLabels);
    testLabels  = onehotEncode(testLabels_one);
elseif strcmp(BENCHMARK, 'LETTER')
    addpath ../RAW_DATA/letter/
    [ trainData, trainLabels, testData, testLabels_one ] = loadDataSet();     % 16000 train data, 4000 test data
    
    trainLabels = onehotEncode(trainLabels);
    testLabels  = onehotEncode(testLabels_one);
elseif strcmp(BENCHMARK, 'CNAE-9')
    addpath ../RAW_DATA/cnae-9
    cnae_9_data = load('CNAE-9.data'); %1080 * 857 double matrix
    [num_data,num_attribute] = size(cnae_9_data);
    Data = cnae_9_data(:,(2:857))';
    Labels = onehotEncode(cnae_9_data(:,1)); %10-bit one-hot-encoding
                    
    trainData = Data(:,(1:927));
    trainLabels = Labels(:,(1:927));
    trainLabels = trainLabels((2:10),:);     % 9 classes [1-9]
    
    testData = Data(:,(928:1080));
    testLabels = Labels(:,(928:1080));
    testLabels = testLabels((2:10),:);       % 9 classes [1-9]
elseif strcmp(BENCHMARK, 'SPAM')
    addpath ../RAW_DATA/spam
    [ trainData, trainLabels, testData, testLabels_one ] = loadDataSet();
    
    trainLabels = onehotEncode(trainLabels);
    testLabels  = onehotEncode(testLabels_one);
end



%% Read error model for approximate multiplier
global err_data;
err_data = load('recover_10_20_forcing_prec_from_0_to_24.mat');


%% MLP training
% initialization & parameter setting for MLP operation
% initialzie the weights (equivalently, theta)
if strcmp(BENCHMARK, 'MNIST')    
    inputSize  = 28*28;    hiddenSize 	= [12*12, 8*8];    outputSize   = 10;
elseif strcmp(BENCHMARK, 'CIFAR10')
    inputSize  = 3*32*32;  hiddenSize   = [32*32];         outputSize   = 10;
elseif strcmp(BENCHMARK, 'LETTER')
    inputSize  = 16;       hiddenSize   = [64];            outputSize   = 26;
elseif strcmp(BENCHMARK,'CNAE-9')
    inputSize  = 856;      hiddenSize   = [12*12, 8*8];    outputSize   = 9;
elseif strcmp(BENCHMARK, 'SPAM')
    inputSize  = 57;       hiddenSize   = [40];            outputSize   = 2;
end


%% MLP training
% initialization & parameter setting for MLP operation
% initialzie the weights (equivalently, theta)
% Added by Duckhwan Kim (150716)
%1. Training bit precision [num_bit, num_bit_for_fractional_part]
global train_prec;

%2. Training convergence condition
global target_cost;
if strcmp(BENCHMARK, 'MNIST')    
    target_cost = 0.01;
elseif strcmp(BENCHMARK, 'CIFAR10')
    target_cost = 0.3;
elseif strcmp(BENCHMARK,'CNAE-9')
    target_cost = 0.03;
elseif strcmp(BENCHMARK,'LETTER')
    target_cost = 0.115;
elseif strcmp(BENCHMARK, 'SPAM')
    target_cost = 0.0455;
end


global N_layer;		% number of MLP layers except input layer
N_layer 	= size(hiddenSize, 2) + 1;

global mat_size;	mat_size = setMatrixSize(inputSize, hiddenSize, outputSize);
initTheta 	= initializeParameters(inputSize);
w_length    = inputSize*hiddenSize(1);

for i = 1:N_layer-2
    w_length = w_length + hiddenSize(i)*hiddenSize(i+1);
end
w_length    = w_length + hiddenSize(N_layer-1)*outputSize;

global network_arch;
network_arch.inputSize  = inputSize;        network_arch.hiddenSize     = hiddenSize;
network_arch.outputSize = outputSize;       network_arch.w_length       = w_length;


%% MLP vectorized code
PRE_TRAIN 	= 1;  	% PRE_TRAIN = 1: perform pre-training (takes long time), PRE_TRAIN = 0: load pre_train.mat file
APPRX 		= 0;	% APPRX = 0: no approximation allowed, APPRX = 1: approximation (on synapses) allowed

if (PRE_TRAIN == 1)
    [ optTheta, grad, iter, cost ] = training_MLP(trainData, trainLabels, initTheta);
%     save('LETTER_trained_data_64.mat', 'optTheta', 'grad');
else
    PT_data     = load('LETTER_trained_data_64.mat');     % load pre-trained data
    optTheta    = PT_data.optTheta;
    grad        = PT_data.grad;
end


%% MLP operation (power constrained NN system)
% operation precision
global prec;        prec = [24 16];
low_prec = prec(1,1);

Acc_Pow     = [12.84e-3,    12.31e-3,   11.51e-3,   10.29e-3,   9.26e-3,    7.96e-3,    6.05e-3];   %32 28 24 20 16 12 8
App_Pow     = [9.16e-3,     8.36e-3,    7.06e-3,    5.52e-3,    3.74e-3,    2.33e-3,    1.03e-3];   %32 28 24 20 16 12 8

global Pow;        Pow  = [Acc_Pow(1), Acc_Pow((32-low_prec)/4+1), App_Pow(1), App_Pow((32-low_prec)/4+1)];

% Recovery for approximated multiplier
global recovery;        recovery = 20;

% approximate synapses (weights)
%     alpha_arr = [0.6:-0.02:0.34];
alpha_arr = [0.9];

N_MC = 1;
MC_ACCURACY = zeros(N_MC,length(alpha_arr));
MC_rPrec    = zeros(N_MC,length(alpha_arr));

for MC_iter = 1:N_MC
    MC_iter
    
    for sim_i = 1:length(alpha_arr)
        alpha  = alpha_arr(sim_i);     % 0.7 means setting 30% power saving as a constraint (should be larger that 0.65)
        
        % choose synapses to be approximated
        rApprx      = 0.4;       % To turn off, approximate multiplier, set rApprx = 0; (this term is lambda in the paper)
        stackedApprxMat = zeros(w_length,1);
        [B, IDX]    = sort(abs(grad(1:w_length)));
        mask_IDX    = IDX(1:round(w_length*rApprx));
        stackedApprxMat(mask_IDX') = 1;
        apprx_mat   = unstackMat(stackedApprxMat);
        
        Ptest   = ((1-rApprx)*Pow(1) + rApprx*Pow(4))*1e3;      % in mW
        Ptarget = alpha * ((1-rApprx)*Pow(1) + rApprx*Pow(3))*1e3;
        
        % P_FLAG = 1: larger than rApprx of synapses need to operate at reduced precision
        if (Ptest < Ptarget)
            P_FLAG = 0;
        else
            P_FLAG = 1;
        end
        
        if ( APPRX ~= 0 )            
            % initialize precision set matrix
            prec_mat = cell(1,N_layer);
            for i = 1:numel(prec_mat)      % initialization
                prec_mat{i} = zeros(mat_size(i,:));
            end
            
            %% 1. Determine Precision Control Rate (rPrec)
            % set ratio of synapses (rApprx) to be approximated
            if (P_FLAG == 0)
                rPrec = round((-Ptarget/1e3+rApprx*Pow(3)+(1-rApprx)*Pow(1))/(Pow(3)-Pow(4))*100)/100;
            else
                rPrec = round((Ptarget/1e3+rApprx*(Pow(2)-Pow(4))-Pow(1))/(Pow(2)-Pow(1))*100)/100;
            end
            
            %% 2. Extracts which synapses to be precision controlled depending on the power
            % budget (alpha needs to be controlled to meet the accuracy as well)
            stackedPrecMat = zeros(w_length,1);
            % [B, IDX] = sort(abs(grad(1:w_length)));
            mask_IDX = IDX(1:round(w_length*rPrec));
            stackedPrecMat(mask_IDX') = 1;
            sprintf('%d%% SYNAPSES ARE PRECISION CONTROLLED',round(length(find(stackedPrecMat == 1))/length(stackedPrecMat)*100))
            
            prec_mat = unstackMat(stackedPrecMat);

        else     % end 'if (APPRX ~= 0)'
            stackedPrecMat = zeros(w_length,1);
            rPrec = 0;
        end
        
        % compute power consumption of current approximated NN (Pcurr)
        Pcurr   = computePower(rPrec, rApprx, P_FLAG)*1e3;      % power (mW) when no synapses are approximated
        
        %% Testing using test MNIST data & trained optTheta        
        if (APPRX == 0)
            [cost,grad,predict] = forwardPass(optTheta, inputSize, hiddenSize, outputSize, testData, testLabels);
        else
            sprintf('RUNNING PRECISION CONTROLLED APPROXIMATE TESTING')
            [cost,predict] = prec_apprx_forwardPass(optTheta, inputSize, hiddenSize, outputSize, testData, testLabels, prec_mat, apprx_mat, 0);
        end
        
        [M, I]		= max(predict);
        
        if strcmp(BENCHMARK, 'CNAE-9')
            pred = I';
            RECOG_RATE(sim_i) = mean(cnae_9_data((928:1080),1) == pred(:))
        else
            pred 		= (I' - 1);
            RECOG_RATE(sim_i) = mean(testLabels_one(:) == pred(:))
        end
        
        avg_Power(sim_i)    = Pcurr
        precRate(sim_i)     = round(length(find(stackedPrecMat == 1))/length(stackedPrecMat)*100);
    end     % end 'sim_i'
       
    MC_ACCURACY(MC_iter,:) = RECOG_RATE;
    MC_rPrec(MC_iter,:)    = precRate;
    
    MEAN_ACCURACY = sum(MC_ACCURACY)/MC_iter
    MEAN_rPrec    = sum(MC_rPrec)/MC_iter
end     % end 'MC_iter'

global result;
result.benchmark = BENCHMARK;
result.train_prec = train_prec;
result.cost_vec(end+1) = cost;
result.iter_vec(end+1) = iter;
result.accuracy_vec(end+1) = RECOG_RATE;
result.grad{length(result.grad)+1} = grad;
result.opt_theta{length(result.opt_theta)+1} = optTheta;
